---
title: "Statistical Models"
subtitle: "Lecture 3"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::


# Lecture 3: <br>Introduction to R & <br> The variance ratio test <br> {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Outline of Lecture 3



::: {.column width="37%"}

1. The basics of R
2. Vectors
3. Graphics
4. Functions

:::



::: {.column width="63%"}

5. The t-test in R
6. One-sample variance ratio test
7. Worked example
8. One-sample variance ratio test in R

:::







# Part 1: <br>The basics of R {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## What is R? {.smaller}

- R is a *high-level* programming language (like **Python**) 
- This means R deals automatically with some details of computer execution:
  * Memory allocation
  * Resources allocation
- R is focused on manipulating and analyzing data



## References {.smaller}
### Slides are based on


::: {.column width="49%"}

- [@dalgaard] Dalgaard, P. 
<br> *Introductory statistics with R*
<br> Second Edition, Springer, 2008   


- [@davies] Davies, T.M.
<br> *The book of R*
<br> No Starch Press, 2016   

:::


::: {.column width="35%"}

**Concise Statistics with R**

<br>

**Comprehensive R manual**

:::





## Installing R on computer {.smaller}

- R is freely available on Windows, Mac OS and Linux
- To install:
  * Download R from CRAN [https://cran.r-project.org](https://cran.r-project.org)
  * Make sure you choose the right version for your system
  * Follow the instructions to install






## How to use R? {.smaller}

- We have installed R. What now?

- Launch the R Console. There are two ways:
  * Find the **R application** on your computer
  * Open a terminal, type **R**, exectute


**Don't have a laptop in class:** Run R code in browser

- [mycompiler.io/new/r](https://www.mycompiler.io/new/r)





## R application {.smaller}
### This is how the R Console looks on the Mac OS app

![](images/R_Console.png){width=82%}



## R from terminal {.smaller}
### This is how the R Console looks on the Mac OS Terminal

![](images/R_Terminal.png){width=82%}




## What can R do?  {.smaller}

- R Console is waiting for commands
- You can use the R Console **interactively**:
    * Type a command after the symbol ``>``
    * Press ``Enter`` to execute
    * R will respond



## Warning  {.smaller}

- The following slides might look like a lot of information
- However you do not have to remember all the material
- It is enough to:
  * Try to understand the examples
  * Know that certain commands exist and what they do
- Combining commands to create complex codes comes with **experience** 



## Simple code can lead to impressive results  {.smaller}

**Example**: Plotting 1000 values randomly generated from $N(0,1)$ distribution 

```{r}
#| echo: true
plot(rnorm(1000))
```




## R as a calculator {.smaller}
### R can perform basic mathematical operations

Below you can see R code and the corresponding answer

```{r}
#| echo: true

2 + 2
2 * 3 - 1 + 2 ^ 7
exp(-10)
log(2)
pi
sin(pi/2)

```




## R Scripts {.smaller}

- The interactive R Console is OK for short codes

- For longer code use **R scripts**
  * Write your code in a text editor
  * Save your code to a **plain text** file with ``.txt`` or ``.R`` extension
  * Execute your code in the R Console with ``source("file_name.R")``

- Examples of text editors
  * TextPad (Windows)
  * TextEdit (MacOS)
  * [VisualStudio Code](https://code.visualstudio.com/docs/languages/r) (Cross platform)




## RStudio {.smaller}



- RStudio is an alternative to R Console and text editors: [Download here](https://posit.co/download/rstudio-desktop/)
  * RStudio is an *Integrated Development Environment (IDE)*
  * It is the R version of *Spyder* for Python



- RStudio includes: 
  * Direct-submission code editor
  * Separate point-and-click panes for files, objects, and project management 
  * Creation of markup documents incorporating R code
  





## Working Directory {.smaller}
### R console

- R session has a working directory associated with it 
- Unless specified, R will use a **default** working directory
- To check the location of the working directory, use the ``getwd`` function
- On my MacOS system I get $\qquad$
![](images/r_working_directory.png){width=32%}

- File paths are always enclosed in double quotation marks
- Note that R uses **forward slashes** (not backslashes) for paths
- You can change the default working directory using the function ```setwd```

```r
setwd("/folder1/folder2/folder3/")

```

- File path can be relative to current working directory or full (system root drive)




## Working Directory {.smaller}
### RStudio


In RStudio you can set the working directory from the menu bar:

- Session ``->`` Set Working Directory ``->`` Choose Directory




## Comments {.smaller}

- Good practice to **document** your code
- This means adding comments directly in the code
- Comments should be brief and explain what a chunk of code does
- To insert a comment, preface the line with a hash mark ``  #``

```{r}
#| echo: true

# This is a comment in R
# Comments are ignored by R

1 + 1   # This works out the result of one plus one!
```





## R Packages {.smaller}

- The base installation of R comes ready with:
  * Commands for numeric calculations
  * Common statistical analyses
  * Plotting and visualization
  
- More specialized techniques and data sets are contained in **packages** (libraries)


```r
# To install a package, run
install.packages("package_name")

# To load a package into your code, type
library("package_name")

# To update all the packages currently installed, type
update.packages()
```





## Help! {.smaller}

- R comes with **help files** that you can use to search for particular functionality
- For example you can check out how to precisely use a given function
- To call for help type ``help(object_name)``


```r
help(mean)     # Mean is R function to compute average of some values
```

![](images/R_help.png){width=62%}




## Further Help {.smaller}

- Sometimes the output of ``help()`` can be cryptic
- Seek help through **Google**
  * Qualify the search with **R** or the name of an R package
  * Paste an error message -- chances are it is common error
- Even better: Search engines specialized for R
  * [search.r-project.org](https://search.r-project.org)
  * [Rseek.org](https://rseek.org)





## Example: Plotting random numbers {.smaller}

Let us go back to the example of the command 
``plot(rnorm(1000))``

The function ``rnorm(n)`` outputs $n$ randomly generated numbers from $N(0,1)$

```{r}
#| echo: true
rnorm(5)
```

The above values can be plotted by concatenating the ``plot`` command

```{r}
#| echo: true
#| output-location: slide
plot(rnorm(5))
```

**Note**: 

- The values plotted (next slide) are, for sure, different from the ones listed above

- This is because every time you call ``rnorm(5)``, new values are generated

- We need to **store** the generated values if we want to re-use them (more later)



## Variables and Assignments {.smaller}

- Values can be stored (assigned) in **symbolic variables** or **objects**
- The **assignment operator** in R is denoted by `` <-``  
(an arrow pointing to the variable to which the value is assigned)


**Example:**

- To assign the value ``2`` to the variable ``x``, enter ``x <- 2``
- To recover the value in ``x``, just type ``x``

```{r}
#| echo: true
# Store 2 in the variable x
x <- 2

# Print x to screen
x
```



## Variables and Assignments {.smaller}

**Continuation of Example:**

- From now on, ``x`` has the value ``2`` 
- The variable ``x`` can be used in subsequent operations
- Such operations do not alter the value of ``x``

```{r}
#| echo: true
# Assign 2 to x
x <- 2

# Compute x + x and print to screen
x + x

# x still contains 2
x
```




## Print and Cat {.smaller}

- If you save the following code in a ``.R`` file and run it,
you will obtain no output

- This is because you need to tell R to print ``x`` to screen

```r
x <- 2
x
```


<br>

- To print a variable to screen use the function ``print()``


```r
x <- 2
print(x)
```
```{r}
x <- 2
print(x)
```



## Print and Cat {.smaller}

- Suppose you wish to print the sentence **Stats is great!** to screen
- To do this, we need to store this sentence in a **string**
- A string is just a sequence of **characters** enclosed by:
  * double-quotations marks
  * or single quotations marks

```r
sentence = "Stats is great!"
```


## Print and Cat {.smaller}

- If now we wish to print the string ``sentence`` to screen we can use

```{r}
#| echo: true
sentence <- "Stats is great!"
print(sentence)
```

<br>

- To avoid R displaying the quotation marks, we can instead
use ``cat()``

```{r}
#| echo: true
sentence <- "Stats is great!"
cat(sentence)
```


## Print and Cat {.smaller}

- ``cat`` can be used to combine strings and variables in a single output

<br>

```{r}
#| echo: true

# Store the result of 2 + 2 in variable two.plus.two

two.plus.two <- 2 + 2

# We want to print to screen the following message:
# "The result of 2 + 2 is two.plus.two"

cat("The result of 2 + 2 is", two.plus.two)
```




## Example - Your first R code {.smaller}

1. Open a text editor and copy paste the below code

```r
# This codes sums two numbers and prints result on screen
x <- 1
y <- 2

result <- x + y

# Print the result on screen
cat("Code run successfully!")
cat("The sum of", x , "and", y, "is", result)
```



## Example - Your first R code {.smaller}

2. Save to a **plain text** file named either
    * ``my_first_code.R``
    * ``my_first_code.txt``

3. Move this file to **Desktop**

4. Open the R Console and change working directory to **Desktop**

```r
# In MacOS type
setwd("~/Desktop")

# In Windows type
setwd("C:/Users/YourUsername/Desktop")
```



## Example - Your first R code {.smaller}

5. Run your code in the R Console by typying either

```r
source("my_first_code.R")
source("my_first_code.txt")
```

6. You should get the following output

```{r}
# This codes sums two numbers and prints result on screen
x <- 1
y <- 2

result <- x + y

# Print the result on screen
cat("Code run successfully!")
cat("The sum of", x , "and", y, "is", result)
```





## The workspace {.smaller}

- Variables created in a session are stored in a **Workspace**
- To display stored variables use 
    * ``ls()``

```{r}
rm(list = ls())
```

```{r}
#| echo: true

# Create 3 variables x, y, z
x <- 2
y <- "Dog"
z <- pi

# Workspace contains variables x, y, z
# This can be displayed by using ls()
ls()
```



## The workspace {.smaller}

You can remove variables from workspace by using 

- ``rm()``

```{r}
#| echo: true

# Create 3 variables x, y, z
x <- 2
y <- "Dog"
z <- pi

# Remove x from workspace
rm(x)

# Now the workspace contains only y and z
ls( )
```




## The workspace {.smaller}

To completely clear the workspace use 

- ``rm(list = ls())``

```{r}
#| echo: true

# Create 3 variables x, y, z
x <- 2
y <- "Dog"
z <- pi

# Remove all variables from workspace
rm(list = ls())

# Let us check that the workspace is empty
ls( )
```




## Saving the Workspace {.smaller}


- You can save the workspace using the command
  * ``save.image("file_name.RData")``

- The file ``file_name.RData``
  * Is saved in the working directory
  * Contains all the objects currently in the workspace

- You can load a saved workspace in a new R session with the command
  * ``load("file_name.RData")``



## Project Management {.smaller}

- Recommended: keep all the files related to a project in a single **folder**

- Such folder will have to be set as working directory in R Console

- Saving the workspace could be **dangerous**
  * This is because R Console automatically loads existing saved workspaces
  * You might forget that this happens, and have undesired objects in workspace
  * This might lead to unintended results

- Always store your code in **R Scripts**




## Exiting R and Saving {.smaller}

To quit the R Console type ``q()``

- You will be asked if you want to save your session
- If you say **YES**, the session will be saved in a ``.RData`` file in the working directory
- Such file will be automatically loaded when you re-open the R Console
- I recommend you **DO NOT** save your session




## Exiting R and Saving {.smaller}
### Summary


- Write your code in **R Scripts**
- These are ``.txt`` or ``.R`` text files
- For later: Data should be stored in ``.txt`` files
- **DO NOT** save your session when prompted







# Part 2: <br>Vectors {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Vectors {.smaller}

- We saw how to store a single value in a variable
- Series of values can be stored in **vectors**
- Vectors can be constructed via the command ``c()``

```{r}
#| echo: true

# Constuct a vector and store it in variable "vector"
vector <- c(60, 72, 57, 90, 95, 72)

# Print vector
print(vector)
```









## Vectorized arithmetic {.smaller}

- A vector is handled by R as a **single** object
- You can do calculations with vectors, as long as they are of the same length
- **Important**: Operations are exectuted component-wise

```{r}
#| echo: true

# Constuct two vectors of radius and height of 6 cylinders
radius <- c(6, 7, 5, 9, 9, 7)
height <- c(1.7, 1.8, 1.6, 2, 1, 1.9)

# Compute the volume of each cylinder and store it in "volume"
volume <- pi * radius^2 * height

# Print volume
print(volume)
```




## Vectorized arithmetic {.smaller}

- If 2 vectors do not have the same length then the shorter vector is **cycled**
- This is called **broadcasting**

```{r}
#| echo: true

a <- c(1, 2, 3, 4, 5, 6, 7)
b <- c(0, 1)

a + b
```

- In the example the vector ``a`` has 7 components while ``b`` has 2 components
- The operation ``a + b`` is executed as follows:
  * ``b`` is copied 4 times to match the length of ``a``
  * ``a + b`` is then obtained by
  $$
  a + \tilde{b} = (1, 2, 3, 4, 5, 6, 7) + (0, 1, 0, 1, 0, 1, 0) = 
  (1, 3, 3, 5, 5, 7, 7)
  $$


## Vectorized arithmetic {.smaller}

Useful applications of broadcasting are:

- Multiplying a vector by a scalar
- Adding a scalar to each component of a vector

```{r}
#| echo: true

vector <- c(1, 2, 3, 4, 5, 6)
scalar <- 2

# Multiplication of vector by a scalar
vector * scalar

# Summing a scalar to each component of a vector
vector + scalar
```



## Sum and length {.smaller}

Two very useful vector operators are:

- ``sum(x)`` which returns the sum of the components of ``x``
- ``length(x)`` which returns the length of ``x``


```r
x <- c(1, 2, 3, 4, 5)

sum <- sum(x)
length <- length(x)

cat("Here is the vector x:", x)
cat("The components of vector x sum to", sum)
cat("The length of vector x is", length)
```

```{r}
x <- c(1, 2, 3, 4, 5)

sum <- sum(x)
length <- length(x)

cat("Here is the vector x: (", x, ")")
cat("The components of vector x sum to", sum)
cat("The length of vector x is", length)
```


## Computing sample mean and variance {.smaller}
### Using vectorized operations

Given a vector $\xx = (x_1,\ldots,x_n)$ we want to compute sample mean and variance
$$
\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i \,, \qquad 
s^2 =  \frac{\sum_{i=1}^n (x_i -  \overline{x})^2 }{n-1}  
$$


```r
# Computing sample mean of vector x
xbar = sum(x) / length(x)


# Computing sample variance of vector x
n = length(x)
s2 = sum( (x - barx)^2 ) / (n - 1) 

```



## Computing sample mean and variance {.smaller}
### Using built in functions

- R is a statistical language
- There are built in functions to compute sample mean and variance:
  * ``mean(x)`` computes the sample mean of ``x``
  * ``sd(x)`` computes the sample standard deviation of ``x``
  * ``var(x)`` computes the sample variance of ``x``




## Exercise {.smaller}
### Computing sample mean and variance

- Let us go back to an Example we saw in Lecure 2
- Below is the Wage data on 10 Mathematicians

|**Mathematician**| $x_1$ | $x_2$| $x_3$ | $x_4$ | $x_5$ | $x_6$ | $x_7$ | $x_8$ | $x_9$ |$x_{10}$|
|:--------------:|:-----:|:----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:----:|:-----:|
|     **Wage**   |   36  |  40  |  46  |  54  |  57  |  58  |  59  |  60  |  62  |  63  |


- In Lecture 2, we have computed $\overline{x}$ and $s^2$ by hand


**Question:** 

1. Enter the data into R
2. Compute $\overline{x}$ and $s^2$ using R



## Solution {.smaller}

```r
# First store the wage data into a vector
x <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)

# Compute the sample mean using formula
xbar = sum(x) / length(x)

# Compute the sample mean using built in R function
xbar_check = mean(x)

# We now print both results to screen
cat("Sample mean computed with formula is", xbar)
cat("Sample mean computed with R function is", xbar_check)
cat("They coincide!")
```

```{r}
# First store the wage data into a vector
x <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)

# Let us compute the mean by using formula
xbar = sum(x) / length(x)

# Compute the mean by using built in R function
xbar_check = mean(x)

# We now print both results to screen
cat("Sample mean computed with formula is", xbar)
cat("Sample mean computed with R function is", xbar_check)
cat("They coincide!")
```



## Solution {.smaller}


```r
# Compute the sample variance using formula
xbar = mean(x)
n = length(x)
s2 = sum( (x - xbar)^2 ) / (n - 1) 

# Compute the sample variance using built in R function
s2_check = var(x)

# We now print both results to screen
cat("Sample variance computed with formula is", s2)
cat("Sample variance computed with R function is", s2_check)
cat("They coincide!")
```

```{r}
# Again, store the wage data into a vector
x <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)

# Compute the sample variance using formula
xbar = mean(x)
n = length(x)
s2 = sum( (x - xbar)^2 ) / (n - 1) 

# Compute the sample variance using built in R function
s2_check = var(x)

# We now print both results to screen
cat("Sample variance computed with formula is", s2)
cat("Sample variance computed with R function is", s2_check)
cat("They coincide!")
```




## Exercise {.smaller}

- R contains some data sets by default, e.g. the data set ``rivers``
    * This data set gives the lengths (in miles) of 141 *major* rivers in North America



- For a vector $x = (x_1,\ldots,x_n) \in \R^n$, the *average distance from the center* is
$$
\frac{|x_1 - \bar{x}| + \ldots + |x_n - \bar{x}|}{n}\,,
$$
where $\bar{x} = \frac{1}{n} \, \sum_{i=1}^n x_i$ is the mean of the vector $x$


**Question:** Compute the average distance from the center for the ``rivers`` data set

*Hint:* The absolute value of $y \in \R$ is computed with ``abs(y)``


## Solution {.smaller}

To compute the average distance from the center for the ``rivers`` data set, we use the following R functions

- ``mean``
- ``sum``
- ``abs``
- ``length``

```{r}
#| echo: true

# Compute the average distance from the center for rivers
sum(abs(rivers - mean(rivers))) / length(rivers)	
```






# Part 3: <br>Graphics {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Graphics {.smaller}

R has extensive built in graphing functions:

- Fancier graphing functions are contained in the library ``ggplot2`` (see [link](https://ggplot2.tidyverse.org))

- However we will be using the basic built in R graphing functions



## Graphics {.smaller}
### Scatter plot


- Suppose given 2 vectors ``x`` and ``y`` of same length 
- The scatter plot of pairs $(x_i,y_i)$ can be generated with ``plot(x, y)``

**Example**: Suppose to have data of **weights** and **heights** of 6 people

- To plot **weight** against **height** code is as follows
- When you run ``plot()`` in R Console the plot will appear in a pop-up window

```{r}
#| echo: true
#| output-location: slide

# Store weight and height data in 2 vectors
weight <- c(60, 72, 57, 90, 95, 72)
height <- c(1.75, 1.80, 1.65, 1.90, 1.74, 1.91)

# Plot weight against height
plot(weight, height)
```






## Graphics {.smaller}
### Scatter plot -- Options


- You can customize your plot in many ways
- Example: you can represent points $(x_i,y_i)$ with triangles instead of circles
- This can be done by including the command 
    * ``pch = 2``
- ``pch`` stands for **plotting character**


```{r}
#| echo: true
#| output-location: slide

# Store weight and height data in 2 vectors
weight <- c(60, 72, 57, 90, 95, 72)
height <- c(1.75, 1.80, 1.65, 1.90, 1.74, 1.91)

# Plot weight against height using little triangles
plot(weight, height, pch = 2)
```





## Graphics {.smaller}
### Plotting 1D function f(x)

- Create a grid of $x$ values
$$
x = (x_1, \ldots, x_n)
$$
- Evaluate $f$ on such grid. This yields a vector
$$
y = (f(x_1), \ldots, f(x_n))
$$
- Generate a scatter plot with 
    * ``plot(x, y)``
- Use the function ``lines`` to linearly interpolate the scatter plot:
    * ``lines(x, y)``





## Graphics {.smaller}
### Plotting functions - Example

Let us plot the parabola
$$
y = x^2 \,, \qquad x \in [-1,1]
$$

```{r}
#| echo: true
#| output-location: slide

# Input vector for grid of x coordinates
x <- c(-1, -0.5, 0, 0.5, 1)

# Compute the function y=x^2 on the grid
y <- x^2

# Generate scatter plot of (x,y)
plot(x, y)

# Add linear interpolation
lines(x, y)
```





## Graphics {.smaller}
### Plotting functions - Example

- The previous plot was quite rough

- This is because we only computed $y=x^2$ on the grid
$$
x = (-1, -0.5, 0, 0.5, 1)
$$
- We could refine the grid by hand, but this is not practical
- To generate a finer grid we can use the built in R function
    * ``seq()``


## Seq function {.smaller}

``seq(from, to, by, length.out)`` generates a vector containing a sequence:

- ``from`` -- The beginning number of the sequence
- ``to`` -- The ending number of the sequence
- ``by`` -- The step-size of the sequence (the increment)
- ``length.out`` -- The total length of the sequence
  
**Example**: Generate the vector of even numbers from 2 to 20 

```{r}
#| echo: true
x <- seq(from = 2, to = 20, by = 2)
print(x)
```


## Seq function {.smaller}

**Note**: The following commands are equivalent:

- ``seq(from = x1, to = x2, by = s)``
- ``seq(x1, x2, s)``


**Example**: Generate the vector of odd numbers from 1 to 11

```r
x <- seq(from = 1, to = 11, by = 2)
y <- seq(1, 11, 2)

cat("Vector x is: (", x, ")")
cat("Vector y is: (", y, ")")
cat("They are the same!")
```


```{r}
x <- seq(from = 1, to = 11, by = 2)
y <- seq(1, 11, 2)

cat("Vector x is: (", x, ")")
cat("Vector y is: (", y, ")")
cat("They are the same!")
```




## Graphics {.smaller}
### Plotting functions - Example 


- We go back to plotting
$$
y = x^2 \,, \qquad x \in [-1, 1]
$$
- We want to generate a grid, or sequece:
  * Starting at $0$
  * Ending at $1$
  * With increments of $0.2$

```{r}
#| echo: true

x <- seq(from = -1, to = 1, by = 0.2)
print(x)
```



## Graphics {.smaller}
### Plotting functions - Example 


```{r}
#| echo: true
#| 
# Use seq() to generate x grid
x <- seq(from = -1, to = 1, by = 0.2)

# Plot the function y=x^2
plot(x, x^2)
lines(x, x^2)
```




## Graphics {.smaller}
### Scatter plot - Example


Let us go back to the example of plotting random normal values

- First we generate a vector ``x`` with 1000 random normal values
- Then we plot ``x`` via ``plot(x)``
- The command ``plot(x)`` implicitly assumes that:
  * ``x`` is the second argument: Values to plot on $y$-axis
  * The first argument is the vector ``seq(1, 1000)``
  * Note that ``seq(1, 1000)`` is the vector of **components numbers** of ``x``

```{r}
#| echo: true
#| output-location: slide

x <- rnorm(1000)

plot(x)
```






# Part 4: <br>Functions in R {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Expressions and objects {.smaller}

- Basic way to interact with R is through **expression evaluation**:
  * You enter an epression
  * The system evaluates it and prints the result

- Expressions work on **objects**
- **Object**: anything that can be assigned to a variable
- Objects encountered so far are:
  * Scalars
  * Vectors



## Functions and arguments {.smaller}

- **Functions** are a class of objects

- Format of a function is **name** followed by parentheses containing **arguments**

- Functions take **arguments** and **return** a result

- We already encountered several built in functions:
  * ``plot(x, y)``
  * ``lines(x, y)``
  * ``seq(x)``
  * ``print("Stats is great!")``
  * ``cat("R is great!")``
  * ``mean(x)``
  * ``sin(x)``


## Functions and arguments {.smaller}

- Functions have **actual arguments** and **formal arguments**
- Example: 
  * ``plot(x, y)`` has formal arguments two vectors ``x`` and ``y``
  * ``plot(height, weight)`` has actual arguments ``height`` and ``weight``

- When you write ``plot(height, weight)`` the arguments are matched:
  * ``height`` corresponds to x-variable
  * ``weight`` corresponds to y-variable
  * This is called **positional matching**
  
  

## Functions and arguments {.smaller}

- If a function has a lot of arguments, positional matching is tedious

- For example ``plot()`` accepts the following (and more!) arguments

| Argument |                        Description    |
|:--------:|:-------------------------------------:|
| ``x``    | x coordinate of points in the plot    |
| ``y``    | y coordinate of points in the plot    |
| ``type`` | Type of plot to be drawn              |
| ``main`` | Title of the plot                     |
| ``xlab`` | Label of x axis                       |
| ``ylab`` | Label of y axis                       | 
| ``pch``  | Shape of points                       |





## Functions and arguments {.smaller}

Issue with having too many arguments is the following:

- We might want to specify ``pch = 2``
- But then we would have to match all the arguments preceding
``pch``
  * ``x``
  * ``y``
  * ``type``
  * ``xlab``
  * ``ylab``



## Functions and arguments {.smaller}

- Thankfully we can use **named actual arguments**:
  * The name of a formal argument can be matched to an actual argument
  * This is independent of position

- For example we can specify ``pch = 2`` by the call
  * ``plot(weight, height, pch = 2)``

- In the above:
  * ``weight`` is implicitly matched to ``x``
  * ``height`` is implicitly matched to ``y``
  * ``pch`` is explicitly matched to ``2``
  
- Note that the following call would give same output
  * ``plot(x = weight, y = height, pch = 2)``






## Functions and arguments {.smaller}

- Named actual arguments **override** positional arguments
- Example: The following commands yield the same plot
  * ``plot(height, weight)``
  * ``plot(x = height, y = weight)``
  * ``plot(y = weight, x = height)``





## Functions and arguments {.smaller}

We have already seen another example of **named actual arguments**

- ``seq(from = 1, to = 11, by = 2)``
- ``seq(1, 11, 2)``
- These yield the same output. Why?
- Because in this case **named actual arguments** match **positional arguments**




## Functions and arguments {.smaller}

If however we want to divide the interval $[1, 11]$ in $5$ equal parts:

- Have to use ``seq(1, 11, length.out = 6)``


```{r}
#| echo: true
seq(1, 11, length.out = 6)
```

- The above is different from ``seq(1, 11, 6)``
```{r}
#| echo: true
seq(1, 11, 6)
```

- They are different because:
  * The 3rd positional argument of ``seq()`` is ``by``
  * Hence the command ``seq(1, 11, 6)`` assumes that ``by = 6``



## Functions and arguments {.smaller}
### Warning

- You can call functions without specifying arguments
- However you have to use brackets ``()``
- Example:
  * ``getwd()`` -- which outputs current working directory
  * ``ls()`` -- which outputs names of objects currently in memory




## Custom functions {.smaller}

- You can define your own functions in R
- Syntax for definining custom function ``my_function`` is below
- You can call your custom function by typing
    * ``my_function(arguments)``

```r
my_function <- function(first = "1st argument", 
                        ... ,
                        nth = "n-th argument") {
  
  # Code here: This is where you tell the function what to do

  return(object)      # Object to be returned  
}
```


## Custom functions -- Example {.smaller}

- The R function ``mean(x)`` computes the sample mean of vector ``x``
- We want to define our own function to compute the mean

- **Example:** The mean of ``x`` could be computed via
    * ``sum(x) / length(x)``
- We want to implement this code into the function ``my_mean(x)``
    * ``my_mean`` takes vector ``x`` as argument
    * ``my_mean`` returns a scalar -- the mean of ``x``

```r
# Definition of custom function my_mean(x)
my_mean <- function(vector = x) {
  
  mean_of_x <- sum(x) / length(x)
  
  return(mean_of_x)  
}
```



## Custom functions -- Example {.smaller}


- Let us use our function ``my_mean`` on an example

```r
# Generate a random vector of 1000 entries from N(0,1)
x <- rnorm(1000)

# Compute mean of x with my_mean
xbar <- my_mean(x)

# Compute mean of x with built in function mean
xbar_check <- mean(x)
  
cat("Mean of x computed with my_mean is:", xbar)
cat("Mean of x computed with R mean is:", xbar_check)
cat("They coincide!")
```

```{r}
my_mean <- function(vector = x) {
  
  mean_of_x <- sum(x) / length(x)
  
  return(mean_of_x)  
}

# Generate a random vector of 1000 entries from N(0,1)
x <- rnorm(1000)

# Compute mean of x with my_mean
xbar <- my_mean(x)

# Compute mean of x with built in function mean
xbar_check <- mean(x)
  
cat("Mean of x computed with my_mean is:", xbar)
cat("Mean of x computed with R mean is:", xbar_check)
cat("They coincide!")
```






# Part 5: <br>The t-test in R {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## The t-test in R {.smaller}

- We are ready to do some statistics in R $\quad \leadsto \quad$ **one-sample t-test**

Suppose given a sample

- $x_1, \ldots, x_n$ from $N(\mu,\sigma^2)$ of size $n$

The *two-sided* hypothesis for testing if $\mu = \mu_0$ is
$$
H_0 \colon \mu = \mu_0 \,, \quad \qquad 
H_1 \colon \mu \neq \mu_0
$$

The *one-sided* alternative hypotheses are
$$
H_1 \colon \mu < \mu_0 \quad  \text{ or } \quad 
H_1 \colon \mu > \mu_0
$$






## Reminder: The t-test by hand {.smaller}

1. **Calculation**: Compute the t-statistic $t = \displaystyle \frac{ \overline{x} - \mu_0}{  s/\sqrt{n} }$  
$\mu_0 = \,$ *null hypothesis* $\qquad$ 
$\overline{x} = \,$ *sample mean* 
$\qquad$ $s = \,$sample *std deviation*

2. **Statistical Tables or R**: Find either
    * Critical value $t^*$ in [Table 1](files/Statistics_Tables.pdf) $\qquad$ or $\qquad$ p-value



3. **Interpretation**: Reject $H_0$ when either
$$
p < 0.05 \qquad \text{ or } \qquad t \in \,\,\text{Rejection Region} 
\qquad \qquad \qquad \qquad
(T \, \sim \, t_{n-1})
$$


| Alternative                      | Rejection Region  | $t^*$             | p-value         |
|----------------------------------|-------------------|-------------------|-----------------|
| $\mu \neq \mu_0$                 | $|t| > t^*$       | $t_{n-1}(0.025)$| $2P(T > |t|)$   |
| $\mu < \mu_0$                    | $t < - t^*$       | $t_{n-1}(0.05)$ | $P(T < t)$      |
| $\mu > \mu_0$                    | $t > t^*$         | $t_{n-1}(0.05)$ | $P(T > t)$      |
: {tbl-colwidths="[25,25,25,25]"}






## The t-test in R {.smaller}


1. Given the sample $x_1,\ldots,x_n$, R can compute the **t-statistic**
$$
t =\frac{\overline x - \mu_0}{s/\sqrt{n}}
$$

2. R can compute the precise p-value (no need for Statistical Tables)
    * If $p < 0.05$ reject $H_0$. The mean is not equal to $\mu_0$


**Note:** The above steps can be done simultaneously by using the command ``t.test``




## The one-sample t-test in R {.smaller}

1. Store the sample $x_1,\ldots,x_n$ in an R vector using  
    * ``x <- c(x1, ..., xn)``

2. Perform a one-sample t-test on ``x`` with null hypothesis $\mu = \mu_0$ using

|Alternative                  |    R command                               |
|-----------------------------|--------------------------------------------|
|$\mu \neq \mu_0$ | ``t.test(x, mu = mu0)``                         |
|$\mu < \mu_0$    |``t.test(x, mu = mu0, alt = "less")`` |
|$\mu > \mu_0$    |``t.test(x, mu = mu0, alt = "greater")`` |
: {tbl-colwidths="[25,75]"}



::: {.column width="100%"}

3. Output contains:

::: {.column width="30%"}

- t-statistic
- degrees of freedom
- p-value

:::

::: {.column width="45%"}

- alternative hypothesis
- confidence interval
- sample mean

:::
:::



## Comments on command ``t.test`` {.smaller}

1. ``mu = mu0 `` tells R to test the hypothesis
$$
H_0 \colon \mu = \mu_0 \,, \qquad H_1 \colon \mu \neq \mu_0
$$

2. If ``mu = mu0 `` is not specified, R assumes $\mu_0 = 0$

3. ``alt`` tells R to perform a one-sided t-test in the specified direction

4. ``conf.level = n `` changes the confidence interval level to ``n`` (default is $0.95$)




## Example: 2008 crisis {.smaller}

Let us go back to the 2008 Crisis example

- **Data:** Monthly Consumer Confidence Index (CCI) in 2007 and 2009
- **Question:** Did the crash of 2008 have lasting impact upon CCI?
- **Observation**: Data shows a massive drop in CCI between 2009 and 2007 
- **Method:** Use $t$-test to see whether there was a change in CCI

| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| CCI 2007                 |86 | 86| 88| 90| 99| 97| 97| 96| 99| 97| 90| 90|
| CCI 2009                 |24 | 22| 21| 21| 19| 18| 17| 18| 21| 23| 22| 21|
| Difference               |62 | 64| 67| 69| 80| 79| 80| 78| 78| 74| 68| 69|




## Setting up the test {.smaller}


- We want to test if there was a change in CCI from 2007 to 2009
- We interested in the difference in CCI


- The **null hypothesis** is that there was (on average) no change in CCI
$$
H_0 \colon \mu = 0 
$$
- The **alternative hypothesis** is that there was some change:
$$
H_1 \colon \mu \neq 0 
$$




## The R code {.smaller}

This is a two-sided t-test. The p-value is computed as

$$
p = 2 P(t_{n-1} > |t|) \,, \qquad t  = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}
$$

```r
# Enter CCI data in 2 vectors using function c()
score_2007 <- c(86, 86, 88, 90, 99, 97, 97, 96, 99, 97, 90, 90)
score_2009 <- c(24, 22, 21, 21, 19, 18, 17, 18, 21, 23, 22, 21)

# Compute vector of differences in CCI
difference <- score_2007 - score_2009

# Perform t-test on difference with null hypothesis mu = 0
# Store output in "answer" and print
answer <- t.test(difference, mu = 0)
print(answer)
```

- The code can be downloaded here [one_sample_t_test.R](codes/one_sample_t_test.R)



## Example: 2008 crisis {.smaller}
### Output of t.test

```{r}
# Enter CCI data using c()
score_2007 <- c(86, 86, 88, 90, 99, 97, 97, 96, 99, 97, 90, 90)
score_2009 <- c(24, 22, 21, 21, 19, 18, 17, 18, 21, 23, 22, 21)

# Compute vector of differences in CCI
difference <- score_2007 - score_2009

# Perform t-test with null hypothesis mu0 = 0
t.test(difference, mu = 0)
```




## Analysis of Output {.smaller}

<br>

```{r}
# Enter CCI data using c()
score_2007 <- c(86, 86, 88, 90, 99, 97, 97, 96, 99, 97, 90, 90)
score_2009 <- c(24, 22, 21, 21, 19, 18, 17, 18, 21, 23, 22, 21)
difference <- score_2007 - score_2009

# Store output of t-test
t_test_result <- t.test(difference, mu = 0)

# Capture output from print
output <- capture.output(print(t_test_result))

# Print only one line of the output
cat(output[2])
cat(output[3])
cat(output[4])
```

<br>

- Description of the test that we have asked for
    * Note: ``t.test`` has automatically assumed that a one-sample test is desired
- This also says which data are being tested 
    * In our case we test the data in ``difference``




## Analysis of Output {.smaller}

<br> 

```{r}
# Enter CCI data using c()
score_2007 <- c(86, 86, 88, 90, 99, 97, 97, 96, 99, 97, 90, 90)
score_2009 <- c(24, 22, 21, 21, 19, 18, 17, 18, 21, 23, 22, 21)
difference <- score_2007 - score_2009

# Store output of t-test
t_test_result <- t.test(difference, mu = 0)

# Capture output from print
output <- capture.output(print(t_test_result))

# Print only one line of the output
cat(output[5])
```

<br>

::: {.column width="48%"}

**This is the best part:**

- $\texttt{t} = \,$ t-statistic from data
- $\texttt{df} = \,$ degrees of freedom
- $\texttt{p-value} = \,$ the exact p-value

:::

::: {.column width="48%"}

**Note:**

- You do not need Statistical Tables! 
- You see that $p < 0.05$ 
- Therefore we reject null hypothesis that the mean difference is $0$

:::



## Analysis of Output {.smaller}

<br> 

```{r}
# Enter CCI data using c()
score_2007 <- c(86, 86, 88, 90, 99, 97, 97, 96, 99, 97, 90, 90)
score_2009 <- c(24, 22, 21, 21, 19, 18, 17, 18, 21, 23, 22, 21)
difference <- score_2007 - score_2009

# Store output of t-test
t_test_result <- t.test(difference, mu = 0)

# Capture output from print
output <- capture.output(print(t_test_result))

# Print only one line of the output
cat(output[6])
```


<br>


- R tells us the alternative hypothesis is $\qquad H_1 \colon \mu \neq 0$
- Hence the Null hypothesis tested is $\qquad \quad H_0 \colon \mu = 0$

- **Warning**: 
    * This message is **not telling you to accept to alternative hypothesis**
    * This message is only **stating the alternative hypothesis**





## Analysis of Output {.smaller}

<br> 

```{r}
# Enter CCI data using c()
score_2007 <- c(86, 86, 88, 90, 99, 97, 97, 96, 99, 97, 90, 90)
score_2009 <- c(24, 22, 21, 21, 19, 18, 17, 18, 21, 23, 22, 21)
difference <- score_2007 - score_2009

# Store output of t-test
t_test_result <- t.test(difference, mu = 0)

# Capture output from print
output <- capture.output(print(t_test_result))

# Print only one line of the output
cat(output[7])
cat(output[8])
```


<br>

$95 \%$ conﬁdence interval for the true mean $\mu$ -- an interval $[a,b]$ s.t.
$$
P(\mu \in [a,b]) \geq 1 - \alpha = 0.95
$$

**Interpretation:** If you repeat the experiment (on new data) over and over, the interval $[a,b]$ will contain $\mu$ about $95\%$ of the times

- Confidence interval is **not** probability statement about $\mu$ -- note $\mu$ is a constant!
- It is probability statement about $[a,b]$ -- these are rv depending on the sample 



## Analysis of Output {.smaller}

**Constructing the confidence interval for t-test:**

- Recall the t-statistic has t-distribution
$$
t = \frac{\overline{x}-\mu}{\ese} \, \sim  \, t_{n-1}
$$

- We impose that $t$ is observed with probability $1-\alpha$ 
$$
P(- t^* \leq t \leq t^*) = 1-\alpha \,, \qquad 
t^* = t_{n-1}(\alpha/2)
$$


- The $1-\alpha$ confidence interval is obtained by solving for $\mu$ 
$$
P(\mu \in [a,b] ) = 1 - \alpha \,, \qquad 
a = \overline{x} - t^* \times \ese , \qquad  
b = \overline{x} + t^* \times \ese
$$



## Analysis of Output {.smaller}


- To obtain $95\%$ confidence, we need $\alpha = 0.05$, so that
$$
1-\alpha = 0.95
$$

- In this case the confidence interval is
$$
\left[ \overline{x} - t^* \times \ese , 
\overline{x} + t^* \times \ese \right] \,, \qquad
t^* = t_{n-1}(0.025)
$$


- R calculated the above for us, giving the confidence interval
$$
\mu \in [68.15960, 76.50706] 
$$


**Interpretation:** If you repeat the experiment (on new data) over and over, the interval $[a,b]$ will contain $\mu$ about $95\%$ of the times




## Analysis of Output {.smaller}

<br> 

```{r}
# Enter CCI data using c()
score_2007 <- c(86, 86, 88, 90, 99, 97, 97, 96, 99, 97, 90, 90)
score_2009 <- c(24, 22, 21, 21, 19, 18, 17, 18, 21, 23, 22, 21)
difference <- score_2007 - score_2009

# Store output of t-test
t_test_result <- t.test(difference, mu = 0)

# Capture output from print
output <- capture.output(print(t_test_result))

# Print only one line of the output
cat(output[9])
cat(output[10])
cat(output[11])
```

<br> 


- This is the sample mean
- You could have easily computed this with the code
    * ``mean(difference)``





## Conclusion {.smaller}


The key information is:

- We conducted a two-sided t-test for the mean difference $\mu \neq 0$
- Results give significant evidence $p<0.05$ that $\mu \neq 0$ 
- The sample mean difference $\overline{x} = 72.33333 \gg 0$
- This suggest CCI mean difference $\mu \gg 0$
- Hence consumer confidence is higher in 2007 than in 2009






## Exercise {.smaller}
### SUV gas mileage

- A consumer group wishes to see whether the actual mileage of a new SUV matches the advertised 17 miles per gallon

- The group suspects it is lower

- To test the claim, the group fills the SUV’s tank and records the mileage

- This is repeated 10 times. The results are below



|       |      |      |      |      |      |      |      |      |      |      |
|-------|------|------|------|------|------|------|------|------|------|------|
|**mpg**| 11.4 | 13.1 | 14.7 | 14.7 | 15.0 | 15.5 | 15.6 | 15.9 | 16.0 | 16.8 |



**Question:** The data is assumed to be normal. Use R to test the claim
$$
H_0 \colon \mu = 17 \,, \qquad 
H_1 \colon \mu < 17
$$



## Solution {.smaller}
### SUV gas mileage


This is a two-sided (left-tailed) t-test. The p-value is computed as

$$
p = P(t_{n-1} < t) \,, \qquad t  = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}
$$


```r
# Enter the mileage data
mpg <- c(11.4, 13.1, 14.7, 14.7, 15.0, 15.5, 15.6, 15.9, 16.0, 16.8)

# Perform one-sided t-test with null hypothesis mu = 17 
# Store answer in "answer"
answer <- t.test(mpg, mu = 17, alternative = "less")

# Print the answer
print(answer)
```



## Solution {.smaller}
### SUV gas mileage


```{r}
# Enter the mileage data
mpg <- c(11.4, 13.1, 14.7, 14.7, 15.0, 15.5, 15.6, 15.9, 16.0, 16.8)

# Perform one-sided t-test with null hypothesis mu = 17 
# Store answer in "answer"
answer <- t.test(mpg, mu = 17, alternative = "less")

# Print the answer
print(answer)
```


**Conclusion:** The p-value is very small $\quad p < 0.05 \quad \implies \quad$ reject $H_0$ 

- The test discredits the claim of 17 miles per gallon
- The SUV is less efficient than advertised





# Part 6: <br>One-sample variance <br> ratio test{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Task: Estimating mean and variance {.smaller}

- Assume the population has normal distribution $N(\mu,\sigma^2)$
    * Mean $\mu$ and variance $\sigma^2$ are **unknown**


- **Questions** about $\mu$ and $\sigma^2$
    1. What is my best guess of the value?
    2. How far away from the true value am I likely to be?


- **Answers:**
    * The one-sample **t-test** answers questions about $\mu$ (seen in Lecture 3)
    * The one-sample **variance ratio test** answers questions about $\sigma^2$





## Reminder {.smaller}

- The-one sample variance test uses **chi-squared distribution**

- **Recall:** Chi-squared distribution with $p$ degrees of freedom is
$$
\chi_p^2 = Z_1^2 + \ldots + Z_p^2
$$
where $Z_1, \ldots, Z_p$ are iid $N(0, 1)$





## One-sample one-sided variance ratio test {.smaller}

**Assumption:** Suppose given sample $X_1,\ldots, X_n$ iid from $N(\mu,\sigma^2)$


**Goal:** Estimate variance $\sigma^2$ of population


**Test:** 

- Suppose $\sigma_0$ is guess for $\sigma$ 

- The one-sided hypothesis test for $\sigma$ is
$$
H_0 \colon \sigma = \sigma_0 \qquad H_1 \colon \sigma > \sigma_0
$$



## What to do? {.smaller}

- Consider the sample variance
$$
S^2 = \frac{ \sum_{i=1}^n  X_i^2 - n \overline{X}^2 }{n-1}
$$

- Since we believe $H_0$, the variance is 
$$
\sigma = \sigma_0
$$

- $S^2$ cannot be too far from the true variance $\sigma$

- Therefore we **cannot** have that 
$$
S^2 \gg \sigma^2 = \sigma_0^2 
$$



## What to do? {.smaller}

- If we observe $S^2 \gg \sigma_0^2$ then our guess $\sigma_0$ is probably wrong

- Therefore we **reject** $H_0$ if
$$
S^2 \gg \sigma_0^2
$$


- The **rejection** condition $S^2 \gg \sigma_0^2$ is equivalent to 
$$
\frac{(n-1)S^2}{\sigma_0^2}  \gg 1
$$
where $n$ is the sample size



## What to do? {.smaller}

- We define our **test statistic** as
$$
\chi^2 := \frac{(n-1)S^2}{\sigma_0^2}
$$


- The **rejection** condition is hence
$$
\chi^2 \gg 1
$$



## What to do? {.smaller}

- In Lecture 2, we have proven that
$$
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$


- Assuming $\sigma=\sigma_0$, we therefore have
$$
\chi^2 = \frac{(n-1)S^2}{\sigma_0^2} = \frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$



## Summary: Rejection condition {.smaller}

- We **reject** $H_0$ if 
$$
\chi^2 = \frac{(n-1)S^2}{\sigma_0^2} \gg 1 
$$

- This means we do not want $\chi^2$ to be too extreme to the right

- As $\chi^2 \sim \chi_{n-1}^2$, we decide to rejct $H_0$ if
$$
\chi^2 > \chi_{n-1}^2(0.05)   
$$


- By definition, the **critical value** $\chi_{n-1}^2(0.05)$ is such that
$$
P(\chi_{n-1}^2 > \chi_{n-1}^2(0.05) ) = 0.05
$$




## Critical values of chi-squared {.smaller}

- $x^* := \chi_{n-1}^2(0.05)$ is point on $x$-axis such that $P(\chi_{n-1}^2 > x^* ) = 0.05$

- Therefore, the semi-open interval $(x^*,+\infty)$ is the *rejection region*

- In the picture we have $n = 12$ and $\chi_{11}^2(0.05) = 19.68$

```{r}
# Degrees of freedom
df <- 11

# Values for x-axis
x <- seq(0, 30, length.out = 1000)  # Adjust the range according to chi-squared distribution

# Calculate PDF of chi-squared distribution
pdf <- dchisq(x, df)

# Plot PDF
plot(x, pdf, type = "l", col = "blue", lwd = 2, xlab = "x", ylab = "Density")


# Shade area where p-value > 0.95
x_fill_right <- x[x >= qchisq(0.95, df)]
y_fill_right <- pdf[x >= qchisq(0.95, df)]
polygon(c(x_fill_right, rev(x_fill_right)), c(y_fill_right, rep(0, length(y_fill_right))), col = "gray", border = NA)

# Add critical value for 0.95 quantile
x_star <- qchisq(0.95, df)
points(x_star, dchisq(x_star, df), col = "red", pch = 19, cex = 1.3)

# Add text above x_star containing its value
text(x_star * 1.05, dchisq(x_star, df) * 1.35, paste("x* =", round(x_star, 2)), pos = 3, col = "red", cex = 1.3)

# Add legend
legend("topright", legend = c("area of rejection region = 0.05"), fill = "gray", cex = 1.3)

```


## Critical values of chi-squared -- Tables {.smaller}

- Find Table 2 in this [file](files/Statistics_Tables.pdf)
- Look at the row with Degree of Freedom $n-1$ (or its closest value)
- Find **critical value** $\chi^2_{n-1}(0.05)$ in column $\alpha = 0.05$
- **Example**: $n=12$, DF $=11$, $\chi^2_{11}(0.05) = 19.68$

![](images/chi_squared_test_statistic_table.png){width=82%}




## The p-value {.smaller}

- Given the test statistic $\chi^2$, the **p-value** is defined as
$$
p := P( \chi_{n-1}^2 > \chi^2 )
$$


- Notice that 
$$
p < 0.05 \qquad \iff \qquad  \chi^2 > \chi_{n-1}^2(0.05)
$$

> This is because $\chi^2 > \chi_{n-1}^2(0.05)$ iff
> $$
> p = P(\chi_{n-1}^2 > \chi^2)
>  < P(\chi_{n-1}^2 > \chi_{n-1}^2(0.05) )
>  = 0.05
> $$




## One-sample one-sided variance ratio test {.smaller}

Suppose given

- Sample $x_1, \ldots, x_n$ of size $n$ from $N(\mu,\sigma^2)$
- Guess $\sigma_0$ for $\sigma$

The one-sided hypothesis test is
$$
H_0 \colon \sigma = \sigma_0 \qquad H_1 \colon \sigma > \sigma_0
$$



## Procedure: 3 steps {.smaller}

1. **Calculation**: Compute the chi-squared statistic
$$
\chi^2 = \frac{(n-1) s^2}{\sigma_0^2}
$$
where sample mean and variance are
$$
\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i \,, \qquad 
s^2 = \frac{\sum_{i=1}^n x_i^2 - n \overline{x}^2}{n-1}
$$



##  {.smaller}

2. **Statistical Tables or R**: Find either
    * Critical value in [Table 2](files/Statistics_Tables.pdf)
    $$
    \chi_{n-1}^2(0.05)
    $$
    * p-value in R
    $$
    p := P( \chi_{n-1}^2 > \chi^2 )
    $$
    (more on this later)

<br>

3. **Interpretation**: Reject $H_0$ if either
$$
\chi^2 > \chi_{n-1}^2(0.05)  \qquad \text{ or } \qquad p < 0.05
$$






# Part 7: <br>Worked example {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## One-sample variance ratio test: Example {.smaller}


| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Cons. Expectation        |66 | 53| 62| 61| 78| 72| 65| 64| 61| 50| 55| 51|
| Cons. Spending           |72 | 55| 69| 65| 82| 77| 72| 78| 77| 75| 77| 77|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|


<br>

- **Data:** *Consumer Expectation* (CE) and *Consumer Spending* (CS) in 2011
- **Assumption:** CE and CS are normally distributed





## One-sample variance ratio test: Example {.smaller}


| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Cons. Expectation        |66 | 53| 62| 61| 78| 72| 65| 64| 61| 50| 55| 51|
| Cons. Spending           |72 | 55| 69| 65| 82| 77| 72| 78| 77| 75| 77| 77|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|

<br>

- **Remark**: Monthly data on CE and CS can be matched 
    * Hence consider: $\quad$ Difference $=$ CE $-$ CS 
    * CE and CS normal $\quad \implies \quad$ Difference $\sim N(\mu,\sigma^2)$

- **Question:** Test the following hypothesis:
$$
H_0 \colon \sigma = 1 \qquad 
H_1 \colon \sigma > 1
$$




## Motivation of test {.smaller}

- If $X \sim N(\mu,\sigma^2)$ then
$$
P( \mu - 2 \sigma \leq X \leq \mu + 2\sigma ) \approx 0.95 
$$

- Recall: $\quad$ Difference $=$ (CE $-$ CS) $\sim N(\mu,\sigma^2)$

- Hence if $\sigma = 1$
$$
P( \mu - 2 \leq {\rm CE} - {\rm CS} \leq \mu + 2 ) \approx 0.95 
$$

- **Meaning of variance ratio test:** 
$$
\sigma=1
\quad \implies \quad 
\text{CS index is within } \pm{2} \text{ of CE index with probability } 0.95  
$$





## The variance ratio test by hand {.smaller}


| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|

<br>

1. **Calculations**: Using the above data, compute 

- Sample mean:
$$
\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n} =  \frac{-6-2-7-4- \ldots -22-26}{12} = -\frac{138}{12} = -11.5
$$




## The variance ratio test by hand {.smaller}

| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|

<br>

1. **Calculations**: Using the above data, compute 

- Sample variance:
\begin{align*}
\sum_{i=1}^{n} x_i^2 & = (-6)^2 + (-2)^2 + (-7)^2 + \ldots + (-22)^2 + (-26)^2 = 2432 \\
s^2 & = \frac{\sum_{i=1}^n x^2_i- n \bar{x}^2}{n-1}
      = \frac{2432-12(-11.5)^2}{11} = \frac{845}{11} = 76.8182
\end{align*}





## The variance ratio test by hand {.smaller}

| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|

<br>

1. **Calculations**: Using the above data, compute 

- Chi-squared statistic:
$$
\chi^2 = \frac{(n-1)s^2}{\sigma_0^2} 
       = \frac{11 \left(\frac{845}{11}\right) }{1} 
       = 845
$$





## The variance ratio test by hand {.smaller}

2. **Statistical Tables**:
    * Sample size is $n = 12$
    * Degrees of freedom are ${\rm df} = n-1 = 11$
    * In [Table 2](files/Statistics_Tables.pdf) find
    $$
    \chi_{11}^2(0.05) = 19.68
    $$

3. **Interpretation**:
    * Test statistic is $\chi^2 = 845$
    * This falls in the *rejection region*
    $$
    \chi^2 = 845 > 19.68 = \chi_{11}^2(0.05)
    $$
    * We **reject** $H_0$



## The variance ratio test by hand {.smaller}

4. **Conclusion**:
    * We reject $H_0$: The standard deviation satisfies $\sigma > 1$
    * A better estimate for $\sigma$ could be sample standard deviation
    $$
    s=\sqrt{\frac{845}{11}}=8.765
    $$
    * This suggests: With probability $0.95$
    $$
    \text{CS index is within } \pm{2 \times 8.765 = \pm 17.53 } \text{ of CE index}  
    $$




# Part 8: <br>One-sample variance <br> ratio test in R{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## The variance ratio test in R {.smaller}

**Goal**: Perform chi-squared variance ratio test in R

- For this, we need to compute p-value
$$
p = P(\chi_{n-1}^2 > \chi^2)
$$

- Thus, we need to compute probabilities for chi-squared distribution in R





## Probability Distributions in R {.smaller}

- R can natively do calculations with known probability distrubutions
- Example: Let $X$ be r.v. with $N(\mu,\sigma^2)$ distribution

| **R command**         |   **Computes**                                |
|:--------------------  |:------------------------------               |
| ``pnorm(x, mean = mu, sd = sig)`` |   $P(X \leq x)$                              |
| ``qnorm(p, mean = mu, sd = sig)`` |   $q$ such that $P(X \leq q) = p$                          |
| ``dnorm(x, mean = mu, sd = sig)`` |   $f(x)$, where $f$ is pdf of $X$    |
| ``rnorm(n, mean = mu, sd = sig)`` |   $n$ random samples from distr. of $X$|
: {tbl-colwidths="[50,50]"}


**Note**: Syntax of commands

**norm** $=$ normal $\qquad$ **p** $=$ probability $\qquad$ **q** $=$ quantile 

**d** $=$ density $\qquad \quad \,\,\,\,$  **r** $=$ random
  


## Example {.smaller}

- Suppose average height of women is normally distributed $N(\mu,\sigma^2)$
- Assume mean $\mu = 163$ cm and standard deviation $\sigma = 8$ cm

<br>

```{r}
#| echo: true

# Probability woman exceeds 180cm in height
# P(X > 180) = 1 - P(X <= 180)

1 - pnorm(180, mean = 163, sd = 8)
```


## {.smaller}

```{r}
#| echo: true

# The upper 10th percentile for women height, that is,
# height q such that P(X <= q) = 0.9

qnorm(0.90, mean = 163, sd = 8)
```

<br>



```{r}
#| echo: true

# Value of pdf at 163

dnorm(163, mean = 163, sd = 8)
```


<br>

```{r}
#| echo: true

# Generate random sample of size 5

rnorm(5, mean = 163, sd = 8)
```


## {.smaller}

**Question:** What is the height of the tallest woman, according to the model?


- The tallest woman could be found using quantiles
- There are roughly 3.5 billion women
- The tallest would be in the top 1/(3.5 billion) quantile

```{r}
#| echo: true

# Find the top 1/(3.5 billion) quantile

p = 1 - 1 / (3.5e9)
qnorm(p, mean = 163, sd = 8)
```


- The current (living) tallest woman is Rumeysa Gelci at $215$ cm ([Wikipedia Page](https://en.m.wikipedia.org/wiki/Rumeysa_Gelgi))




## Probability Distributions in R {.smaller}
### Chi-squared distribution

- Commands for chi-squared distrubution are similar
- ``df = n`` denotes $n$ degrees of feedom

| **R command**         |   **Computes**                                |
|:--------------------  |:------------------------------               |
| ``pchisq(x, df = n)`` |   $P(X \leq x)$                              |
| ``qchisq(p, df = n)`` |   $q$ such that $P(X \leq q) = p$                          |
| ``dchisq(x, df = n)``  |   $f(x)$, where $f$ is pdf of $X$    |
| ``rchisq(m, df = n)``  |   $m$ random samples from distr. of $X$|
: {tbl-colwidths="[40,60]"}



## Example 1 {.smaller}

- From Tables we found quantile $\chi_{11}^2 (0.05) = 19.68$
- **Question:** Compute such quantile in R





## Example 1 -- Solution {.smaller}

- From Tables we found quantile $\chi_{11}^2 (0.05) = 19.68$
- **Question:** Compute such quantile in R


<br>

```{r}
#| echo: true
# Compute 0.95 quantile for chi-squared with 11 degrees of freedom

quantile <- qchisq(0.95, df = 11)

cat("The 0.95 quantile for chi-squared with df = 11 is", quantile)
```


## Example 2 {.smaller}

- The $\chi^2$ statistic for variance ratio test has distribution $\chi_{n-1}^2$

- **Question:** Compute the p-value
$$
p := P(\chi_{n-1}^2 > \chi^2)
$$



## Example 2 -- Solution {.smaller}

- The $\chi^2$ statistic for variance ratio test has distribution $\chi_{n-1}^2$

- **Question:** Compute the p-value
$$
p := P(\chi_{n-1}^2 > \chi^2)
$$


- Observe that
$$
p := P(\chi_{n-1}^2 > \chi^2) = 1 - P(\chi_{n-1}^2  \leq  \chi^2)
$$

- The code is therefore

```r
# Compute p-value for chi^2 = chi_squared and df = n

p_value <- 1 - pchisq(chi_squared, df = n)
```




## The variance ratio test in R {.smaller}


| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Cons. Expectation        |66 | 53| 62| 61| 78| 72| 65| 64| 61| 50| 55| 51|
| Cons. Spending           |72 | 55| 69| 65| 82| 77| 72| 78| 77| 75| 77| 77|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|

<br>

- **Back to the Worked Example:** Monthly data on CE and CS

- **Question:** Test the following hypothesis:
$$
H_0 \colon \sigma = 1 \qquad 
H_1 \colon \sigma > 1
$$




## The variance ratio test in R {.smaller}

| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Cons. Expectation        |66 | 53| 62| 61| 78| 72| 65| 64| 61| 50| 55| 51|
| Cons. Spending           |72 | 55| 69| 65| 82| 77| 72| 78| 77| 75| 77| 77|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|

<br>

- Start by entering data into R

```r
# Enter Consumer Expectation and Consumer Spending data
CE <- c(66, 53, 62, 61, 78, 72, 65, 64, 61, 50, 55, 51)
CS <- c(72, 55, 69, 65, 82, 77, 72, 78, 77, 75, 77, 77)

# Compute difference
difference <- CE - CS
``` 



## The variance ratio test in R {.smaller}

- Compute chi-squared statistic
$$
\chi^2 = \frac{(n-1) s^2}{\sigma^2_0}
$$


```r
# Compute sample size
n <- length(difference)

# Enter null hypothesis
sigma_0 <- 1

# Compute sample standard deviation
s <- sd(difference)

# Compute chi-squared statistic
chi_squared <- (n - 1) * s ^ 2 / sigma_0 ^ 2
```



## The variance ratio test in R {.smaller}

- Compute the p-value, and print to screen
$$
p = P(\chi_{n-1}^2 > \chi^2) = 1 - P(\chi_{n-1}^2 \leq \chi^2)
$$


```r
# Compute p-value
p_value <- 1 - pchisq(chi_squared, df = n - 1)

# Print p-value
cat("The p-value for one-sided variance test is", p_value)
```

<br>

- The full code can be downloaded here [variance_ratio_test.R](codes/variance_ratio_test.R)



## Running the code {.smaller}

- Running [variance_ratio_test.R](codes/variance_ratio_test.R) gives the following output:
```{r}
# Enter Consumer Expectation and Consumer Spending data
CE <- c(66, 53, 62, 61, 78, 72, 65, 64, 61, 50, 55, 51)
CS <- c(72, 55, 69, 65, 82, 77, 72, 78, 77, 75, 77, 77)

# Compute difference
difference <- CE - CS

# Compute sample size
n <- length(difference)

# Enter null hypothesis
sigma_0 <- 1

# Compute sample standard deviation
s <- sd(difference)

# Compute chi-squared statistic
chi_squared <- (n - 1) * s ^ 2 / sigma_0 ^ 2

# Compute p-value
p_value <- 1 - pchisq(chi_squared, df = n - 1)

# Print p-value
cat("The p-value for one-sided variance test is", p_value)
```

<br>

- Since $p = 0 < 0.05$ we **reject** $H_0$
- Therefore the true variance seems to be $\sigma^2 > 1$




## References