---
title: "Statistical Models"
subtitle: "Lecture 5"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 5: <br>Two-sample F-test & <br> Goodness-of-fit test  {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::






## Outline of Lecture 5

1. The goodness-of-fit test
2. Worked Examples 






# Part 1: <br> The goodness-of-fit test {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Scenario 1: Simple counts {.smaller}

**Data:** in the form of **numerical counts**

**Test:** difference between observed counts and predictions of theoretical model

**Example**: Blood counts

- We conducted blood type testing on a sample of 6004 individuals, and the results are summarized below.

    | A  |  B  |  AB  |  O  |
    |:--:|:---:|:----:|:---:|
    |2162| 738 | 228  |2876 |

- We want to compare the above data to the theoretical probability model

    | A  |  B  |  AB  |  O  |
    |:--:|:---:|:----:|:---:|
    |1/3 | 1/8 |  1/24| 1/2 |




## Scenario 2: Counts with multiple factors {.smaller}

| Manager     |     Won |  Drawn  | Lost    |
|:-----------:|:-------:|:-------:|:-------:|
| **Moyes**   |   27    | 9       |   15    |
|**Van Gaal** |   54    |  25     |   24    |
|**Mourinho** |   84    |  32     |   28    |
|**Solskjaer**|    91   |   37    |   40    |
|**Rangnick** |    11   |   10    |   8     |
|**ten Hag**  |    61   |   12    |   28    |


**Example:** Relative performance of Manchester United managers

- Each football manager has Win, Draw and Loss count



## Scenario 2: Counts with multiple factors {.smaller}

| Manager     |     Won |  Drawn  | Lost    |
|:-----------:|:-------:|:-------:|:-------:|
| **Moyes**   |   27    | 9       |   15    |
|**Van Gaal** |   54    |  25     |   24    |
|**Mourinho** |   84    |  32     |   28    |
|**Solskjaer**|    91   |   37    |   40    |
|**Rangnick** |    11   |   10    |   8     |
|**ten Hag**  |    61   |   12    |   28    |


**Questions:**

- Is the number of Wins, Draws and Losses uniformly distributed?
- Are there differences between the performances of each manager?



## Plan {.smaller}

1. In this Lecture: 
    * We study Scenario 1 -- Simple counts
    * Chi-squared goodness-of-fit test

2. Next week:
    * We will study Scenario 2 -- Counts with multiple factors
    * Chi-squared test of independence



## Categorical Data {.smaller}


- Finite number of possible categories or types
- Observations can only belong to one category
- $O_i$ refers to **observed count** of category $i$

| Type $1$| Type $2$ |  $\ldots$ | Type $n$ |
|:-------:|:--------:|:---------:|:--------:|
|$O_1$    |   $O_2$  | $\ldots$  |    $O_n$ |

- $E_i$ refers to **expected count** of category $i$

| Type $1$| Type $2$ |  $\ldots$ | Type $n$ |
|:-------:|:--------:|:---------:|:--------:|
|$E_1$    |   $E_2$  | $\ldots$  |    $E_n$ |
 


## Chi-squared goodness-of-fit test {.smaller}

**Goal:** Compare expected counts $E_i$ with observed counts $O_i$


**Null hypothesis:** Expected counts match the Observed counts

$$
H_0 \colon O_i = E_i \,, \qquad \forall \, i = 1, \ldots, n
$$

**Method:** Look for evidence against the null hypothesis

- Distance between observed counts and expected counts is large

- For example, if
$$
(O_i - E_i)^2 \geq c
$$
for some chosen constant $c$



## Chi-squared statistic {.smaller}

::: Definition

The **chi-squared statistic** is
$$
\chi^2 := \sum_{i=1}^n \frac{(O_i-E_i)^2}{E_i}
$$

:::

**Remark:** 

- $\chi^2$ represents the global distance between observed and expected counts

- Indeed, we have that
$$
\chi^2 = 0 \qquad \iff \qquad O_i = E_i  \,\,\,\, \text{ for all } \,\,\,\, i = 1 , \, \ldots , \, n
$$


## Tests using Chi-squared statistic {.smaller}

**Null hypothesis:** Expected counts match the Observed counts

$$
H_0 \colon O_i = E_i \,, \qquad \forall \, i = 1, \ldots, n
$$

**Remarks:**

- If $H_0$ is to be believed, we expect small differences between $O_i$ and $E_i$ 
    * Therefore $\chi^2$ will be small (and non-negative)
 
- If $H_0$ is wrong, it will happen that some $O_i$ are larger than the $E_i$
    * Therefore $\chi^2$ will be large (and non-negative)

- The above imply that tests on $\chi^2$ should be **one-sided** (right-tailed)





## The Multinomial distribution {.smaller}

Models the following experiment
  
- The experiment consists of $m$ independent trials 
- Each trial results in one of $n$ distinct possible outcomes
- The probability of the $i$-th outcome is $p_i$ on every trial, with
$$
0 \leq p_i \leq 1 \qquad \qquad \sum_{i=1}^n p_i = 1
$$
- $X_i$ counts the number of times $i$-th outcome occurred in the m trials. It holds
$$
\sum_{i=1}^n X_i = m
$$



## Multinomial distribution {.smaller}
### Schematic visualization

<br>


|  Outcome type | $1$  | $\ldots$ | $n$   | Total                   |
|:-------------|:----:|:--------:|:-----:|:-----------------------:|
| Counts        |$X_1$ | $\ldots$ | $X_n$ | $X_1 + \ldots + X_n = m$|
| Probabilities |$p_1$ | $\ldots$ | $p_n$ | $p_1 + \ldots + p_n = 1$|
: {tbl-colwidths="[20,10,10,10,30]"}





## The case $n = 2$ {.smaller}

For $n = 2$, the multinomial reduces to a binomial:

- Each trial has $n = 2$ possible outcomes
- $X_1$ counts the number of *successes* 
- $X_2 = m − X_1$ counts the number of *failures* in $m$ trials
- Probability of *success* is $p_1$
- Probability of *failure* is $p_2 = 1 - p_1$


| Outcome types | $1$     | $2$               |
|:------------- |:-------:|:-----------------:|
| Counts        |$X_1$    |  $X_2 = m - X_1$  |
| Probabilities |$p_1$    |  $p_2 = 1 - p_1$  |
: {tbl-colwidths="[20,20,30]"}




## Formal definition {.smaller}

::: Definition

Let $m,n \in \N$ and $p_1, \ldots, p_n$ numbers such that
$$
0 \leq p_i \leq 1 \,, \qquad \quad 
\sum_{i=1}^n p_i = 1
$$
The random vector $\XX = (X_1, \ldots, X_n)$ has **multinomial distribution** with $m$ trials and cell probabilities $p_1,\ldots,p_n$ if joint pmf is
$$
f (x_1, \ldots , x_n) = \frac{m!}{x_1 ! \cdot \ldots \cdot x_n !} \ p_1^{x_1} \cdot \ldots \cdot p_n^{x_n}  \,, \qquad \forall \, x_i \in \N  \, \st \sum_{i=1}^n x_i = m
$$
We denote $\XX \sim \multinomial(m,p_1,\ldots,p_n)$

:::



## Properties of Multinomial distribution {.smaller}

Suppose that $\XX = (X_1, \ldots, X_n) \sim \multinomial(m,p_1,\ldots,p_n)$

- If we are only interested in outcome $i$, the remaining outcomes are *failures*

- This means $X_i$ is binomial with $m$ trials and *success* probability $p_i$  

- We write $X_i \sim \binomial(m,p_i)$ and the pmf is
$$
f(x_i) = P(X = x_i) = \frac{m!}{x_i! \cdot (1-x_i)!} \, p_i^{x_i} (1-p_i)^{1-x_i} 
\qquad \forall \, x_i = 0 , \ldots , m 
$$
- Since $X_i \sim \binomial(m,p_i)$ it holds
$$
\Expect[X_i] = m p_i \qquad \qquad 
\Var[X_i] = m p_i (1-p_i)
$$



## Statistical Model: Multinomial Counts {.smaller}

- $O_i$ refers to **observed count** of category $i$


- $E_i$ refers to **expected count** of category $i$

- We suppose that Type $i$ is observed with probability $p_i$ and
$$
0 \leq p_i \leq 1 \,, \qquad \quad p_1 + \ldots + p_n = 1
$$

- Total number of observations is $m$

- The counts are modelled by
$$
(O_1, \ldots, O_n) \sim \multinomial (m, p_1, \ldots, p_n)
$$

- The expected counts are modelled by
$$
E_i := \Expect[ O_i ] = m p_i
$$





## The chi-squared statistic {.smaller}

Consider counts and expected counts
$$
(O_1, \ldots, O_n) \sim \multinomial (m, p_1, \ldots, p_n)
\qquad \qquad 
E_i := m p_i
$$


::: Definition

The chi-squared statistic for multinomial counts is defined by
$$
\chi^2 = \sum_{i=1}^n \frac{(O_i-E_i)^2}{E_i} 
       = \sum_{i=1}^n \frac{( O_i - m p_i )^2}{ m p_i }
$$

:::

**Question**: What is the distribution of $\chi^2 \,$?




## Distribution of chi-squared statistic {.smaller}

::: Theorem

Suppose the counts $(O_1, \ldots, O_n) \sim \multinomial (m,p_1, \ldots, p_n)$. Then
$$
\chi^2 = \sum_{i=1}^n \frac{( O_i - m p_i )^2}{ m p_i } \ \stackrel{{\rm d}}{\longrightarrow} \ \chi_{n-1}^2
$$
when sample size $m \to \infty$, where the convergence is in distribution
:::

- Hence, the distribution of $\chi^2$ is **approximately** $\chi_{n-1}^2$ when $m$ is large

- The above Theorem is due to Karl Pearson in his 1900 paper [link](https://www.tandfonline.com/doi/abs/10.1080/14786440009463897)

- Proof is difficult. Seven different proofs are presented in this paper [link](https://arxiv.org/abs/1808.09171)



## Heuristic proof of Theorem {.smaller}

- Since $O_i \sim \binomial(m, p_i)$, the *Central 
Limit Theorem* implies
$$
 \frac{O_i - \Expect[O_i]}{ \sqrt{\Var[O_i] } } = 
\frac{O_i - m p_i }{ \sqrt{m p_i(1 - p_i) } } \
 \stackrel{{\rm d}}{\longrightarrow} \ N(0,1)
$$
as $m \to \infty$

- In particular, since $(1-p_i)$ in constant, we have
$$
\frac{O_i - m p_i }{ \sqrt{m p_i } } \ \approx \ \frac{O_i - m p_i }{ \sqrt{m p_i(1 - p_i) } } \ \approx \ N(0,1) 
$$




## Heuristic proof of Theorem {.smaller}

- Squaring the previous expression, we get
$$
 \frac{(O_i - m p_i)^2 }{ m p_i } \ \approx   \ N(0,1)^2 = \chi_1^2  
$$


- If the above random variables were pairwise independent, we would obtain
$$
\chi^2 = \sum_{i=1}^n \frac{(O_i - m p_i)^2 }{ m p_i } \ \approx  \ \sum_{i=1}^n \chi_1^2 = \chi_n^2  
$$


- However the $O_i$ are not independent, because of the linear constraint
$$
O_1 + \ldots + O_n = m
$$
(total counts have to sum to $m$)



## Heuristic proof of Theorem {.smaller}


- A priori, there should be $n$ degrees of freedom

- However, the linear constraint reduces degrees of freedom by $1$
    * because one can choose the first $n-1$ counts, and the last one is given by
$$
O_n = m - O_1 - \ldots - O_{n-1}
$$

- Thus, we have $n-1$ degrees of freedom, implying that
$$
\chi^2 = \sum_{i=1}^n \frac{(O_i - m p_i)^2 }{ m p_i } \ \approx \ \chi_{n-1}^2  
$$

**This is not a proof! The actual proof is super technical**



## Quality of chi-squared approximation {.smaller}

- Define expected counts $E_i := m p_i$ 
- Consider approximation from Theorem:
$$
\chi^2 = \sum_{i=1}^n \frac{(O_i - E_i)^2 }{ E_i } \ \approx \ \chi_{n-1}^2  
$$

- The approximation is:
  
  | **Good**     |  $E_i \geq 5  \, \text{ for all } \, i = 1 , \ldots n$ |
  |:------------ |:------------------------------------------------------ |
  | **Bad**      |                $E_i < 5 \,$ for some $\, i = 1 , \ldots n$ |
    

- **How to compute the p-value:** When approximation is
    * **Good:** Use $\chi_{n-1}^2$ approximation of $\chi^2$
    * **Bad:** Use Monte Carlo simulations (more on this later)





## The chi-squared goodness-of-fit test {.smaller}

**Setting:** 

- Population consists of individuals of $n$ different types
- $p_i$ is probability that an individuals selected at random is of type $i$


**Problem:** $p_i$ is unknown and needs to be estimated

**Hypothesis:** As guess for $p_1,\ldots,p_n$, we take $p_1^0, \ldots, p_n^0$ such that
$$
0 \leq p_i^0 \leq 1 \qquad \qquad \sum_{i=1}^n p_i^0 = 1
$$



## The chi-squared goodness-of-fit test {.smaller}

**Formal Hypothesis:** We test for equality of $p_i$ to $p_i^0$
\begin{align*}
& H_0 \colon p_i = p_i^0 \qquad \text{ for all } \, i = 1, \ldots, n \\
& H_1 \colon p_i \neq p_i^0 \qquad \text{ for at least one } \, i 
\end{align*}


**Sample**: 

- We draw $m$ items from population
- $O_i$ denotes the number of items of type $i$ drawn
- According to our model, 
$$
(O_1, \ldots, O_n) \sim \multinomial (m,p_1, \ldots, p_n)
$$




## The chi-squared goodness-of-fit test {.smaller}

**Data**: Vector of counts $(o_1,\ldots,o_n)$

**Schematically:** We can represent probabilities and counts in a table

<br>

|  Type      | $1$  | $\ldots$ | $n$   | Total                   |
|:----------- |:----:|:--------:|:-----:|:-----------------------:|
| Counts      |$o_1$ | $\ldots$ | $o_n$ | $m$|
|Probabilities|$p_1$ | $\ldots$ | $p_n$ | $1$|



## Procedure: 3 Steps {.smaller}

1. **Calculation**: 
    * Compute total counts and expected counts
    $$
    m = \sum_{i=1}^n o_i \qquad \quad E_i = m p_i^0
    $$ 
    * Compute the chi-squared statistic
    $$
    \chi^2 = \sum_{i=1}^n \frac{ (o_i - E_i)^2 }{E_i}
    $$



##  {.smaller}

2. **Statistical Tables or R**: 
    * Check that $E_i \geq 5$ for all $i = 1, \ldots, n$
    * In this case $\chi^2 \ \approx \ \chi_{n-1}^2$
    * Find critical value $\chi^2_{n-1}(0.05)$ in [Table 2](files/Statistics_Tables.pdf)
    * Alternatively, compute p-value in R

<br>

3. **Interpretation**: Reject $H_0$ when either
$$
p < 0.05 \qquad \text{ or } \qquad \chi^2 \in \,\,\text{Rejection Region}
$$


| Alternative                                      | Rejection Region               |    p-value                 |
|--------------------------------------------------|--------------------------------|----------------------------|
| $\exists \, i \,\,$ s.t. $\,\, p_i \neq p_i^0$ | $\chi^2 > \chi^2_{n-1}(0.05)$  | $P(\chi_{n-1}^2 > \chi^2)$ |
: {tbl-colwidths="[28,28,20]"}
 




## {.smaller}

```{r}
# Degrees of freedom
df <- 3

# Values for x-axis
x <- seq(0, 10, length.out = 1000)

# Calculate PDF of F-distribution
pdf <- dchisq(x, df)


# ==========================
# Right-tailed test
# ==========================

plot(x, pdf, type = "l", col = "blue", lwd = 2, xlab = "", ylab = "",
     main = expression("Alternative: " ~ p[i] != p[i]^0))

# Critical value for 0.05 significance level
crit_right_tail <- qchisq(0.95, df)

# Shading rejection region
x_fill_right <- x[x >= crit_right_tail]
y_fill_right <- pdf[x >= crit_right_tail]
polygon(c(x_fill_right, rev(x_fill_right)), c(y_fill_right, rep(0, length(y_fill_right))),
        col = "gray", border = NA)

# Add critical value label with symbolic subscripts
text(crit_right_tail + 0.8, dchisq(crit_right_tail, df) +0.01,
     expression(chi[n-1](0.05)), pos = 3, col = "red", cex = 1.1)

# Legend
legend("topright", legend = c("Rej. Region area = 0.05"), fill = "gray", cex = 1)


```


Reject $H_0$ if $\chi^2$ statistic falls in the *Rejection Region* (in gray)





## The chi-squared goodness-of-fit test in R {.smaller}

1. Store the counts $o_1,\ldots, o_n$ in R vector
    * ``counts <- c(o1, ..., on)``

2. Store the null hypothesis probabilities $p_1^0,\ldots, p_n^0$ in R vector
    * ``null.p <- c(p1, ..., pn)``

3. Perform a goodness-of-fit test on ``counts`` with ``null.p``

|Alternative                                    |    R command                         |
|-----------------------------------------------|--------------------------------------|
|$\exists \, i \,\,$ s.t. $\,\, p_i \neq p_i^0$ | ``chisq.test(counts, p = null.p)``  |
: {tbl-colwidths="[30,70]"}


4. Read output: similar to two-sample t-test and F-test
    * The main quantity of interest is p-value





## Comment 1 {.smaller}
###  Default null probabilities

If null probabilities are not specified:

- R assumes equal probability for each type

- For example, assume that ``counts <- c(o1, ..., on)``

- Equal probability means that
$$
p_i^0 = \frac{1}{n} \,, \qquad \forall \, i = 1, \ldots, n
$$

- Test ``counts`` against equal probabilities with the command 
    * ``chisq.test(counts)`` 



## Comment 2 {.smaller}
###  Quality of chi-squared approximation

- By default, R computes p-value via the $\chi_{n-1}^2$ approximation

- **R will warn you if expected counts are too low, and tell you to use Monte Carlo**


- To compute p-value via Monte Carlo simulation, use option
    * ``simulate.p.value = T``  






# Part 2: <br>Worked Examples {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Example 1: Blood counts {.smaller}

- Suppose to have **counts** of blood type for some people
- We also have theoretical probabilities that we want to test

    |Blood type     | A  |  B  |  AB  |  O  |
    |:--------------|:--:|:---:|:----:|:---:|
    |**Count**      |2162| 738 | 228  |2876 |
    |**Probability**|1/3 | 1/8 |  1/24| 1/2 |


- Hence the null hypothesis probabilities are 
$$
p_1^0 = \frac{1}{3} \qquad \quad   p_2^0 = \frac{1}{8} \qquad \quad
p_3^0 = \frac{1}{24} \qquad \quad p_4^0 = \frac{1}{2}
$$

- The alternative hypothesis is: $\qquad H_1 \colon p_i \neq p_i^0 \,\,$ for at least one $\, i$



## Goodness-of-fit test by hand {.smaller}

1. **Calculation**: 
    * Compute total counts
    $$
    m = \sum_{i=1}^n o_i = 2162 + 738 + 228 + 2876 = 6004
    $$
    * Compute expected counts
    \begin{align*}
    E_1 & = m p_1^0 = 6004 \times \frac{1}{3} = 2001.3  \\ 
    E_2 & = m p_2^0 = 6004 \times \frac{1}{8} = 750.5   \\
    E_3 & = m p_3^0 = 6004 \times \frac{1}{24} = 250.2   \\
    E_4 & = m p_4^0 = 6004 \times \frac{1}{2} = 3002  
    \end{align*}


##  {.smaller}

1. **Calculation**: 
    * Compute the chi-squared statistic
    \begin{align*}
    \chi^2 & = \sum_{i=1}^n \frac{ (o_i - E_i)^2 }{E_i} \\
           & = \frac{ (2162 − 2001.3)^2 }{ 2001.3 } + 
               \frac{ (738 − 750.5)^2 }{ 750.5 } \\
           & \phantom{ = } + \frac{ (228 − 250.2)^2 }{ 250.2 } + 
               \frac{ (2876 − 3002)^2 }{ 3002 } \\    
           & = 20.36  
    \end{align*}





## {.smaller}

2. **Statistical Tables**: 
    * Degrees of freedom are $\, {\rm df} = n - 1 = 3$
    * We have computed
    $$
    E_1 = 2001.3 \qquad E_2 = 750.5 \qquad E_3 = 250.2 \qquad E_4 = 3002
    $$
    * Hence $E_i \geq 5$ for all $i = 1, \ldots, n$
    * In this case $\chi^2 \ \approx \ \chi_{3}^2$
    * In chi-squared [Table 2](files/Statistics_Tables.pdf) we find critical value
    $$
    \chi^2_{3} (0.05) = 7.82
    $$
    



##  {.smaller}

3. **Interpretation:**
    * We have that
    $$
    \chi^2 = 20.36  > 7.82 = \chi_{3}^2 (0.05)
    $$
    * Therefore we reject $H_0$
    * This means we accept the alternative
    $$
    H_1 \colon p_i \neq p_i^0 \qquad \text{ for at least one } \, i
    $$

**Conclusion:** Observed counts suggest at least one of null probabilities $p_i^0$ is wrong



## Goodness-of-fit test in R {.smaller}

- We use R to perform a goodness-of-fit test on alternative
$$
H_1 \colon p_i \neq p_i^0 \qquad \text{ for at least one } \, i
$$

- The code can be downloaded here [good_fit.R](codes/good_fit.R)

```r
# Enter counts and null hypothesis probabilities
counts <- c(2162, 738, 228, 2876)
null.p <- c(1/3, 1/8, 1/24, 1/2)

# Perform goodness-of-fit test
# Store the output and print

ans <- chisq.test(counts, p = null.p)
print(ans)
```



## Output {.smaller}

- Running the code in the previous slide we obtain


```{r}
# Enter counts and null hypothesis probabilities
counts <- c(2162, 738, 228, 2876)
null.p <- c(1/3, 1/8, 1/24, 1/2)

# Perform goodness-of-fit test
# Store the output and print

ans <- chisq.test(counts, p = null.p)
print(ans)
```

<br>


::: {.column width="49%"}

- Chi-squared statistic is $\, \chi^2 = 20.359$

- Degrees of freedom are $\, {\rm df} = 3$

- p-value is $p \approx 0$

:::


::: {.column width="35%"}

- Therefore $p < 0.05$ 

- We reject $H_0$

:::


**Conclusion:** At least one of the theoretical probabilities appears to be wrong




## Example 2: Fair dice {.smaller}

- A dice with $6$ faces is rolled $100$ times
- The counts observed are

|Outcome  | $1$ | $2$ | $3$  | $4$ |  $5$|  $6$|
|:--      |:---:|:---:|:----:|:---:|:---:|:---:|
|**Count**|13   | 17  | 9    | 17  | 18  | 26  |


- **Exercise**: Is the dice fair?
    * Formulate appropriate goodness-of-fit test
    * Implement this test in R




## Solution {.smaller}

- The dice is fair if 

$$
P(\text{rolling }\, i) = \frac16 \,, \quad \forall \, i = 1, \ldots, 6
$$

- Therefore, we test the following hypothesis
\begin{align*}
& H_0 \colon p_i = \frac{1}{6} \qquad \text{ for all } \, i = 1, \ldots, 6 \\
& H_1 \colon p_i \neq \frac{1}{6} \qquad \text{ for at least one } \, i 
\end{align*}



## {.smaller}

- The R code is given below

```r
# Enter counts and null hypothesis probabilities
counts <- c(13, 17, 9, 17, 18, 26)
null_p. <- rep(1/6, 6)

# Perform goodness-of-fit test
chisq.test(counts, p = null.p)
```

<br> 

- Note that each type is equally likely

- Therefore, we can achieve same result **without** specifying null probabilities


```r
# Enter counts
counts <- c(13, 17, 9, 17, 18, 26)

# Perform goodness-of-fit test assuming equal probabilities
chisq.test(counts)
```


## Solution {.smaller}

- Both codes in the previous slide give the following output


```{r}
counts <- c(13, 17, 9, 17, 18, 26)
chisq.test(counts)
```

<br>


::: {.column width="49%"}

- Chi-squared statistic is $\, \chi^2 = 9.68$

- Degrees of freedom are $\, {\rm df} = 5$

- p-value is $p \approx 0.08$

:::


::: {.column width="35%"}

- Therefore $p > 0.05$ 

- We cannot reject $H_0$


:::

**Conclusion:** The dice appears to be fair




## Example 3: Voting data {.smaller}

- Assume there are two candidates: Republican and Democrat
- Voter can choose one of these, or be undecided
- 100 people are surveyed, and the results are 

| Republican | Democrat | Undecided |
|:----------:|:--------:|:---------:|
|   35       |   40     |   25      |


- Hystorical data suggest that undecided voters are $30\%$ of population

- **Exercise**: Is difference between Republican and Democratic significant?
    * Formulate appropriate goodness-of-fit test
    * Implement this test in R
    * You are not allowed to use ``chisq.test``



## Solution {.smaller}

- Hystorical data suggest that undecided voters are $30\%$ of population. Hence
$$
p_3^0 = 0.3
$$
- Want to test if there is difference between Republican and Democrat
- Hence null hypothesis is
$$
p_1^0 = p_2^0
$$

- Since probabilities must sum to 1, we get
$$
p_1^0 + p_2^0 + p_3^0 = 1 \quad \implies \quad 
p_1^0 = 0.35\,, \qquad  p_2^0 = 0.35 \,, \qquad 
p_3^0 = 0.3
$$

- We test the hypothesis
$$
H_0 \colon p_i = p_i^0 \,\, \text{ for all }  \, i = 1, \ldots, 3 \,, 
\qquad H_1 \colon  p_i \neq p_i^0 \, \text{ for some }\, i
$$


## {.smaller}

- Perform goodness-of-fit test without using ``chisq.test``

- First, we enter the data

```r
# Enter counts and null hypothesis probabilities
counts <- c(35, 40 , 25)
null.p <- c(0.35, 0.35, 0.3)
```

- Compute the total number of counts $m = o_1 + \ldots + o_n$

```r
# Compute total counts
m <- sum(counts)
```

- Compute degrees of freedom $\, {\rm df} = n - 1$

```r
# Compute degrees of freedom
degrees <- length(counts) - 1
```



##  {.smaller}

- Compute the expected counts $E_i = m p_i^0$

```r
# Compute expected counts
exp.counts <- m * null.p
```

- We now check that the expected counts satisfy $E_i \geq 5$

```r
# Print expected counts with a message
cat("The expected counts are:", exp.counts)

# Check if the expected counts are larger than 5
if (all(exp.counts >= 5)) {
  cat("Expected counts are larger than 5.", 
      "\nThe chi-squared approximation is valid!")
} else {
  cat("Warning, low expected counts.",
      "\nMonte Carlo simulation must be used.")
}
```

```{r}
counts <- c(35, 40 , 25)
null.p <- c(0.35, 0.35, 0.3)
m <- sum(counts)
exp.counts <- m * null.p

# Print expected counts with a message
cat("The expected counts are:", exp.counts)

# Check if the expected counts are larger than 5
if (all(exp.counts >= 5)) {
  cat("Expected counts are larger than 5.", 
      "\nThe chi-squared approximation is valid!")
} else {
  cat("Warning, low expected counts.",
      "\nMonte Carlo simulation must be used.")
}
```


##  {.smaller}

- Compute the chi-squared statistic 
$$
\chi^2 = \sum_{i=1}^n \frac{( o_i - E_i )^2}{E_i}
$$



```r
# Compute chi-squared statistic
chi.squared <- sum( (counts - exp.counts)^2 / exp.counts )
```

- We know that all the counts are larger than 5. Thus $\chi^2 \approx \chi_{n-1}^2$

- We can therefore compute the p-valued with the formula
$$
p = P( \chi_{n-1}^2 > \chi^2 ) = 1 - P( \chi_{n-1}^2 \leq \chi^2 )
$$

```r
# Compute p-value
p_value <- 1 - pchisq(chi.squared, df = degrees)

# Print p-value
cat("The p-value is:", p_value)
```




##  {.smaller}

- The full code can be downloaded here [good_fit_first_principles.R](codes/good_fit_first_principles.R)

- Running the code gives the following output

```{r}
# Enter counts and null hypothesis probabilities
counts <- c(35, 40 , 25)
null.p <- c(0.35, 0.35, 0.3)

# Compute total counts
m <- sum(counts)

# Compute degrees of freedom
degrees <- length(counts) - 1

# Compute expected counts
exp.counts <- m * null.p

# Print expected counts with a message
cat("The expected counts are:", exp.counts)

# Check if the expected counts are larger than 5
if (all(exp.counts >= 5)) {
  cat("Expected counts are larger than 5.", 
      "\nThe chi-squared approximation is valid!")
} else {
  cat("Warning, low expected counts.",
      "\nMonte Carlo simulation must be used.")
}

# Compute chi-squared statistic
chi.squared <- sum( (counts - exp.counts)^2 / exp.counts )

# Compute p-value
p_value <- 1 - pchisq(chi.squared, df = degrees)

# Print p-value
cat("The p-value is:", p_value)
```

<br>

- Therefore p-value is $p > 0.05$

- We do not reject $H_0$


**Conclusion:** No reason to believe that Republicans and Democrats are not tied

